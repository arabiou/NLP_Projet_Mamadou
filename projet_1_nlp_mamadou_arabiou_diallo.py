# -*- coding: utf-8 -*-
"""Projet_NLP_Mamadou_Arabiou_DIALLO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XigXXpAKINHIkbxzB7gta-4lpQ7KUPiI

## Projet 1 : Topic Modeling des Avis des Produits

Installer SpaCy et télécharger le modèle de langue anglais
"""

!pip install spacy
!python -m spacy download en_core_web_sm

import spacy

# Charger le modèle de langue anglaise
nlp = spacy.load("en_core_web_sm")

"""# PARTIE I : Prétraitement des Avis de Produits"""

from google.colab import files

# Téléchargez les fichiers JSONL depuis local vers Google Colab
uploaded = files.upload()

# Afficher les noms des fichiers téléchargés
print(uploaded)

import pandas as pd

# Charger les fichiers JSONL dans des DataFrames pandas
meta_data = pd.read_json('meta.jsonl', lines=True)
reviews_data = pd.read_json('reviews.jsonl', lines=True)

# Afficher un aperçu des données
print("Aperçu des métadonnées :")
print(meta_data.head())

print("\nAperçu des avis clients :")
print(reviews_data.head())

"""1- Traitement linguistique en synthese"""

def preprocess_text(text, nlp_model):
    """
    Applique le prétraitement linguistique :
    - Tokenisation
    - Lemmatisation
    - Suppression des stop words
    - Suppression de la ponctuation et des éléments non pertinents
    """
    # Appliquer SpaCy au texte
    doc = nlp_model(text.lower())  # Convertir le texte en minuscule

    # Filtrer les tokens (lemmatisation et exclusion des stopwords et ponctuation)
    tokens = [
        token.lemma_ for token in doc
        if not token.is_stop and not token.is_punct and token.is_alpha
    ]

    # Retourner les tokens sous forme de texte
    return " ".join(tokens)

# Appliquer le prétraitement à la colonne 'text' des avis
reviews_data['cleaned_text'] = reviews_data['text'].apply(lambda x: preprocess_text(x, nlp))

# Afficher un aperçu des avis nettoyés
print("Aperçu des avis après prétraitement :")
print(reviews_data[['title', 'cleaned_text']].head())

"""2.Traitement linguistique

2-1 Tokenisation

Selection du premier avis client
"""

sample_text = reviews_data['text'].iloc[0]  # Prendre le premier avis

# Appliquer SpaCy pour obtenir les tokens
doc = nlp(sample_text)

# Extraire les tokens
tokens = [token.text for token in doc]

# Afficher les tokens
print("Tokens :")
print(tokens)

"""2-2 Lemmatisation"""

# Appliquer la lemmatisation sur les tokens
lemmatized_tokens = [token.lemma_ for token in doc]

# Afficher les tokens après lemmatisation
print("Lemmatisation :")
print(lemmatized_tokens)

"""2-3 Suppression des stop words"""

# Filtrer les tokens pour supprimer les stop words
filtered_tokens = [token.lemma_ for token in doc if not token.is_stop]

# Afficher les tokens après suppression des stop words
print("Après suppression des stop words :")
print(filtered_tokens)

"""2-4 Exclusion des éléments non pertinents"""

# Filtrer les tokens pour exclure la ponctuation et les termes non alphabétiques
cleaned_tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.is_alpha]

# Afficher les tokens après exclusion des éléments non pertinents
print("Après exclusion des éléments non pertinents (ponctuation, chiffres, etc.) :")
print(cleaned_tokens)

from collections import Counter

# Compter la fréquence des mots après nettoyage
word_freq = Counter(cleaned_tokens)

# Afficher les 10 mots les plus fréquents
print("20 mots les plus fréquents après prétraitement :")
print(word_freq.most_common(20))

# Appliquer le traitement complet (lemmatisation, suppression des stop words, exclusion des éléments non pertinents)
def preprocess_to_tokens(text, nlp_model):
    """
    Prétraitement pour générer une liste de tokens filtrés :
    - Lemmatisation
    - Suppression des stop words
    - Exclusion de la ponctuation et des termes non pertinents
    """
    doc = nlp_model(text.lower())
    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and token.is_alpha]
    return tokens

# Appliquer le prétraitement à la colonne 'text' des avis
reviews_data['filtered_tokens'] = reviews_data['text'].apply(lambda x: preprocess_to_tokens(x, nlp))

# Afficher un aperçu des données après ajout des tokens
print("Aperçu des données avec tokens filtrés :")
print(reviews_data[['title', 'filtered_tokens']].head())

# Sauvegarder les listes de tokens dans un fichier JSON
reviews_data[['title', 'filtered_tokens']].to_json('filtered_reviews.json', orient='records', lines=True)

print("Les données préparées ont été sauvegardées dans le fichier 'filtered_reviews.json'.")

from google.colab import files

# Télécharger le fichier JSON
files.download('filtered_reviews.json')

"""# PAETIE II - Clustering non supervisé des documents pour identifier des topics et mots-clés

1.Génération des embeddings
"""

import json

# Charger le fichier JSON contenant les tokens pré-traités
with open('filtered_reviews.json', 'r') as file:
    documents = [json.loads(line)['filtered_tokens'] for line in file]

# Vérification
print("Exemple de document pré-traité :")
print(documents[0])

# Installer SentenceTransformers
!pip install sentence-transformers

from sentence_transformers import SentenceTransformer

# Charger le modèle SentenceTransformers
model = SentenceTransformer('all-MiniLM-L6-v2')

# Convertir chaque document en une chaîne de texte avant de générer les embeddings
text_documents = [" ".join(tokens) for tokens in documents]

# Générer les embeddings
embeddings = model.encode(text_documents)

print("Taille des embeddings générés :")
print(embeddings.shape)

from sklearn.feature_extraction.text import TfidfVectorizer

# Initialiser le vecteur TF-IDF
tfidf_vectorizer = TfidfVectorizer()

# Générer la matrice TF-IDF
tfidf_matrix = tfidf_vectorizer.fit_transform(text_documents)

print("Dimensions de la matrice TF-IDF :")
print(tfidf_matrix.shape)

"""Commentaires : La matrice TF-IDF de dimensions (1000, 2620) indique qu'il y a 1000 documents et 2620 termes uniques après prétraitement. Cela montre un vocabulaire riche et une bonne granularité pour appliquer des méthodes comme le clustering ou l'analyse thématique.

2.Clustering

Clustering avec KMeans
"""

from sklearn.cluster import KMeans

# Définir le nombre de clusters
num_clusters = 5

# Initialiser et ajuster le modèle KMeans
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
kmeans.fit(tfidf_matrix)

# Obtenir les labels des clusters
kmeans_labels = kmeans.labels_

# Ajouter les clusters aux documents
reviews_data['cluster'] = kmeans_labels

print("Distribution des documents par cluster (KMeans) :")
print(reviews_data['cluster'].value_counts())

"""Commentaires : La distribution des documents par cluster montre que les clusters 4, 2, 1 et 3 regroupent la majorité des documents, avec des tailles respectives décroissantes. Cependant, le cluster 0 contient seulement 6 documents, ce qui pourrait indiquer un groupe d'avis très spécifiques ou des données bruitées."""

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# Appliquer PCA pour réduire la dimension à 2
pca = PCA(n_components=2)
pca_components = pca.fit_transform(tfidf_matrix.toarray())

# Ajouter les composantes PCA aux données
reviews_data['pca1'] = pca_components[:, 0]
reviews_data['pca2'] = pca_components[:, 1]

# Visualiser les clusters avec un graphique
plt.figure(figsize=(10, 8))
plt.scatter(reviews_data['pca1'], reviews_data['pca2'], c=reviews_data['cluster'], cmap='viridis', marker='o')
plt.title('Visualisation des Clusters (KMeans) avec PCA')
plt.xlabel('Composante principale 1')
plt.ylabel('Composante principale 2')
plt.colorbar(label='Cluster')
plt.show()

from sklearn.cluster import DBSCAN
from sklearn.metrics.pairwise import cosine_similarity

# Utiliser une mesure de similarité (DBSCAN fonctionne mieux avec des distances)
cosine_sim_matrix = cosine_similarity(tfidf_matrix)

# Initialiser et ajuster le modèle DBSCAN
dbscan = DBSCAN(metric='precomputed', eps=0.5, min_samples=5)
dbscan_labels = dbscan.fit_predict(cosine_sim_matrix)

# Ajouter les clusters aux documents
reviews_data['dbscan_cluster'] = dbscan_labels

print("Distribution des documents par cluster (DBSCAN) :")
print(reviews_data['dbscan_cluster'].value_counts())

from sklearn.neighbors import NearestNeighbors
import numpy as np
import matplotlib.pyplot as plt

# Définir min_samples pour DBSCAN
min_samples = 5

# Calculer les distances des k plus proches voisins
nearest_neighbors = NearestNeighbors(n_neighbors=min_samples)
nearest_neighbors.fit(tfidf_matrix)
distances, indices = nearest_neighbors.kneighbors(tfidf_matrix)

# Trier les distances pour visualiser le coude
distances = np.sort(distances[:, -1])
plt.plot(distances)
plt.title('Graphe des distances des k-voisins')
plt.xlabel('Points (triés)')
plt.ylabel('Distance au k-voisin')
plt.show()

from sklearn.cluster import DBSCAN

# Paramètres ajustés
adjusted_eps = 0.8  # Remplacez par la valeur trouvée
adjusted_min_samples = 5

# Réexécution de DBSCAN
dbscan_adjusted = DBSCAN(metric='cosine', eps=adjusted_eps, min_samples=adjusted_min_samples)
dbscan_labels_adjusted = dbscan_adjusted.fit_predict(tfidf_matrix)

# Ajouter les labels ajustés au DataFrame
reviews_data['dbscan_adjusted_cluster'] = dbscan_labels_adjusted

print("Distribution des documents par cluster (DBSCAN ajusté) :")
print(reviews_data['dbscan_adjusted_cluster'].value_counts())

"""3.Analyse des clusters"""

# Associer les documents aux clusters obtenus par KMeans
reviews_data['cluster'] = kmeans_labels  # Utilisez kmeans_labels pour les clusters KMeans

# Regrouper les documents par cluster
clustered_reviews = reviews_data.groupby('cluster')['text'].apply(list)

# Afficher un exemple de documents pour chaque cluster
for cluster_num, docs in clustered_reviews.items():
    print(f"Cluster {cluster_num}: {len(docs)} documents")
    print("Exemple de documents :")
    print(docs[:2])  # Afficher les deux premiers documents de chaque cluster
    print("\n")

"""Fréquences des mots dans chaque clusters et les 10 mots les plus fréquents"""

from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd
import numpy as np

# Initialiser le CountVectorizer pour obtenir la fréquence brute des mots (en excluant les mots fonctionnels)
vectorizer = CountVectorizer(stop_words='english')

# Dictionnaire pour stocker la matrice de fréquence de mots pour chaque cluster
word_frequencies_per_cluster = {}

# Calculer la fréquence des mots pour chaque cluster
for cluster_num, docs in clustered_reviews.items():
    # Calculer la fréquence des mots dans les documents du cluster
    word_frequencies_per_cluster[cluster_num] = vectorizer.fit_transform(docs)

    # Convertir la matrice de fréquence en un DataFrame pour une meilleure lisibilité
    word_freq_df = pd.DataFrame(word_frequencies_per_cluster[cluster_num].toarray(), columns=vectorizer.get_feature_names_out())

    # Calculer la fréquence totale pour chaque mot dans le cluster
    word_freq_sum = word_freq_df.sum(axis=0).sort_values(ascending=False)

    # Afficher les 10 mots les plus fréquents dans le cluster
    print(f"Cluster {cluster_num} - Fréquence des mots :")
    print(word_freq_sum.head(10))  # Afficher les 10 mots les plus fréquents
    print("\n")

from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd

# Initialiser le CountVectorizer pour extraire les bigrams (deux mots consécutifs)
vectorizer = CountVectorizer(stop_words='english', ngram_range=(2, 2))

# Dictionnaire pour stocker la matrice de fréquence de bigrams pour chaque cluster
bigram_frequencies_per_cluster = {}

# Calculer la fréquence des bigrams pour chaque cluster
for cluster_num, docs in clustered_reviews.items():
    # Calculer la fréquence des bigrams dans les documents du cluster
    bigram_frequencies_per_cluster[cluster_num] = vectorizer.fit_transform(docs)

    # Convertir la matrice de fréquence en un DataFrame pour une meilleure lisibilité
    bigram_freq_df = pd.DataFrame(bigram_frequencies_per_cluster[cluster_num].toarray(), columns=vectorizer.get_feature_names_out())

    # Calculer la fréquence totale pour chaque bigram dans le cluster
    bigram_freq_sum = bigram_freq_df.sum(axis=0).sort_values(ascending=False)

    # Afficher les 10 bigrams les plus fréquents dans le cluster
    print(f"Cluster {cluster_num} - Fréquence des bigrams :")
    print(bigram_freq_sum.head(10))  # Afficher les 10 bigrams les plus fréquents
    print("\n")

"""3- Vérification de pertinence : Analysez si ces mots fréquents sont représentatifs du
thème ou du sujet du cluster.

Cluster 0 : Accessoires pour téléphones

Mots fréquents : "cable", "usb", "phone", "car", "update".
Analyse : Ce cluster est probablement centré sur des avis concernant des câbles, chargeurs, ou des accessoires spécifiques utilisés dans des véhicules.

Cluster 1 : Étuis et coques de téléphone

Mots fréquents : "phone", "case", "fit", "cover", "like".
Analyse : Ce cluster traite des étuis ou coques pour téléphones, en mettant l'accent sur l'ajustement et la compatibilité avec les appareils.

Cluster 2 : Téléphones et chargeurs

Mots fréquents : "screen", "charge", "charger", "phone", "works".
Analyse : Ce cluster semble se concentrer sur les problèmes d'écran et de charge des téléphones, avec un intérêt particulier pour la fonctionnalité des appareils.

Cluster 3 : Coques de protection pour téléphones (iPhone)

Mots fréquents : "case", "phone", "iphone", "protection", "good".
Analyse : Ce cluster concerne les coques de protection, en particulier pour les iPhones, avec des avis généralement positifs.

Cluster 4 : Produits électroniques divers

Mots fréquents : "great", "like", "easy", "love", "band".
Analyse : Ce cluster semble lié à des accessoires électroniques comme des bracelets ou des produits similaires, avec une forte satisfaction client.


En fin, Les clusters analysés représentent des thèmes distincts : accessoires pour téléphones, coques de protection, problèmes liés à la charge ou à l'écran, et produits électroniques divers. Les mots fréquents dans chaque cluster sont pertinents pour les thèmes observés, ce qui indique une bonne qualité de séparation des avis par type de produit.

● Évaluation des clusters
"""

from sklearn.metrics import silhouette_score

"""Calculer le Silhouette Score pour le clustering KMeans

"""

# Calcul du Silhouette Score pour le clustering KMeans
sil_score = silhouette_score(tfidf_matrix, kmeans.labels_)
print(f"Silhouette Score pour KMeans : {sil_score}")

"""# Étape III : Analyse des sentiments des avis clients

1- Chargement et préparation des données
"""

import pandas as pd

# Charger les fichiers JSONL dans des DataFrames pandas
meta_data = pd.read_json('meta.jsonl', lines=True)
reviews_data = pd.read_json('reviews.jsonl', lines=True)

# Afficher un aperçu des données
print("Aperçu des métadonnées :")
print(meta_data.head())

print("\nAperçu des avis clients :")
print(reviews_data.head())

"""Extraction des notes"""

# Extraire les avis (reviewText) et les notes (rating)
texts = reviews_data['title'].values  # Texte des avis
ratings = reviews_data['rating'].values  # Notes des clients

# Afficher quelques exemples d'avis et de notes
print("\nExemples d'avis et de notes :")
for i in range(5):
    print(f"Avis: {texts[i]} | Note: {ratings[i]}")

"""2- Chargement du modèle pré-entraîné"""

from transformers import pipeline

# Charger le modèle de sentiment
sentiment_model = pipeline('sentiment-analysis', model='nlptown/bert-base-multilingual-uncased-sentiment')

# Analyse des sentiments
reviews_data['sentiment'] = reviews_data['text'].apply(lambda x: sentiment_model(x[:512])[0]['label'])

from scipy.stats import pearsonr

# Conversion des sentiments en scores numériques
sentiment_scores = reviews_data['sentiment'].apply(lambda x: int(x[0]))
correlation, _ = pearsonr(reviews_data['rating'], sentiment_scores)
print(f"Correlation entre les notes réelles et prédictions: {correlation}")

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt

# Calcul des métriques
accuracy = accuracy_score(reviews_data['rating'], sentiment_scores)
precision = precision_score(reviews_data['rating'], sentiment_scores, average='weighted')
recall = recall_score(reviews_data['rating'], sentiment_scores, average='weighted')
f1 = f1_score(reviews_data['rating'], sentiment_scores, average='weighted')

# Affichage des résultats
print(f"Accuracy : {accuracy:.2f}")
print(f"Precision : {precision:.2f}")
print(f"Recall : {recall:.2f}")
print(f"F1 Score : {f1:.2f}")

import seaborn as sns
import matplotlib.pyplot as plt

# Distribution des sentiments
sns.countplot(x='sentiment', data=reviews_data)
plt.title('Distribution des sentiments')
plt.show()

sentiment_counts = reviews_data['sentiment'].value_counts(normalize=True) * 100
plt.figure(figsize=(8, 5))
sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, palette="viridis")
plt.title('Proportion des sentiments (%)')
plt.ylabel('Pourcentage')
plt.xlabel('Sentiment')
plt.show()

from wordcloud import WordCloud

# Filtrer les avis positifs et négatifs
positive_reviews = ' '.join(reviews_data[reviews_data['sentiment'] == '5 stars']['text'])
negative_reviews = ' '.join(reviews_data[reviews_data['sentiment'] == '1 star']['text'])

# Générer les Word Clouds
positive_cloud = WordCloud(width=800, height=400, background_color='white').generate(positive_reviews)
negative_cloud = WordCloud(width=800, height=400, background_color='black').generate(negative_reviews)

# Afficher les Word Clouds
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.imshow(positive_cloud, interpolation='bilinear')
plt.title('Mots fréquents dans les avis positifs (5 étoiles)')
plt.axis('off')

plt.subplot(1, 2, 2)
plt.imshow(negative_cloud, interpolation='bilinear')
plt.title('Mots fréquents dans les avis négatifs (1 étoile)')
plt.axis('off')

plt.show()

"""Division des données en lots (batches) à l’aide d’un outil comme un DataLoader"""

from torch.utils.data import Dataset, DataLoader
import torch

class ReviewDataset(Dataset):
    def __init__(self, texts, ratings, tokenizer, max_length=512):
        self.texts = texts
        self.ratings = ratings
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        rating = self.ratings[idx]

        # Tokenisation du texte
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'rating': torch.tensor(rating, dtype=torch.long)
        }

# Créer le DataLoader
batch_size = 16
dataset = ReviewDataset(texts, ratings, tokenizer)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Afficher le premier lot
for batch in dataloader:
    print(batch)
    break

"""5. Évaluation des performances :

● Associez les sentiments prédits (1 étoile à 5 étoiles) à des scores numériques (de 1 à 5)

● Corrélation : Calculez la corrélation de Pearson entre les notes réelles et les prédictions.
"""

# Conversion des sentiments en scores numériques
# Exemple : '5 stars' -> 5
reviews_data['predicted_score'] = reviews_data['sentiment'].apply(lambda x: int(x[0]))

# Affichage des colonnes pertinentes pour vérification
print(reviews_data[['rating', 'sentiment', 'predicted_score']].head())

from scipy.stats import pearsonr

# Calcul de la corrélation de Pearson
correlation, p_value = pearsonr(reviews_data['rating'], reviews_data['predicted_score'])

# Affichage des résultats
print(f"Corrélation entre les notes réelles et prédictions : {correlation:.2f}")
print(f"P-value associée : {p_value:.3e}")

"""Commentaires :  

Une corrélation de 0.81 indique une relation linéaire forte et positive entre les notes réelles et les prédictions.
Cela signifie que les prédictions du modèle suivent généralement la tendance des notes réelles.

Une p-value extrêmement faible (≈ 0) indique que la corrélation observée est hautement statistiquement significative.

Matrice de confusion
"""

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Calcul de la matrice de confusion
conf_matrix = confusion_matrix(reviews_data['rating'], reviews_data['predicted_score'])

# Visualisation avec seaborn
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[1, 2, 3, 4, 5], yticklabels=[1, 2, 3, 4, 5])
plt.title('Matrice de confusion des prédictions')
plt.xlabel('Prédictions')
plt.ylabel('Vérité terrain')
plt.show()

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

# Calcul des métriques
accuracy = accuracy_score(reviews_data['rating'], reviews_data['predicted_score'])
precision = precision_score(reviews_data['rating'], reviews_data['predicted_score'], average='weighted')
recall = recall_score(reviews_data['rating'], reviews_data['predicted_score'], average='weighted')
f1 = f1_score(reviews_data['rating'], reviews_data['predicted_score'], average='weighted')

# Affichage des résultats
print(f"Accuracy : {accuracy:.2f}")
print(f"Precision : {precision:.2f}")
print(f"Recall : {recall:.2f}")
print(f"F1 Score : {f1:.2f}")

import matplotlib.pyplot as plt
import numpy as np

# Calcul des fréquences pour les notes réelles et les scores prédits
real_counts = reviews_data['rating'].value_counts().sort_index()
predicted_counts = reviews_data['predicted_score'].value_counts().sort_index()

# Liste des scores (de 1 à 5)
scores = np.arange(1, 6)

# Création du graphique en barres
plt.figure(figsize=(10, 6))

# Barres pour les notes réelles
plt.bar(scores - 0.2, real_counts[scores], width=0.4, alpha=0.7, label='Notes réelles', color='blue', align='center')

# Barres pour les scores prédits
plt.bar(scores + 0.2, predicted_counts[scores], width=0.4, alpha=0.7, label='Scores prédits', color='orange', align='center')

# Ajouter des titres, des labels et une légende
plt.xlabel('Scores (1 à 5)', fontsize=12)
plt.ylabel('Nombre d\'avis', fontsize=12)
plt.title('Comparaison entre Notes Réelles et Scores Prédits', fontsize=14)
plt.xticks(scores)
plt.legend(fontsize=12)
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Affichage du graphique
plt.show()

import matplotlib.pyplot as plt

# Création du scatter plot
plt.figure(figsize=(8, 6))
plt.scatter(reviews_data['rating'], reviews_data['predicted_score'], alpha=0.5, color='blue')

# Ajouter des titres et des labels
plt.title('Comparaison des Notes Réelles et des Scores Prédits', fontsize=16)
plt.xlabel('Notes Réelles', fontsize=12)
plt.ylabel('Scores Prédits', fontsize=12)

# Affichage de la ligne idéale (si le modèle prédit parfaitement)
plt.plot([1, 5], [1, 5], color='red', linestyle='--', label='Ligne idéale')

# Affichage de la légende
plt.legend()

# Affichage du plot
plt.show()

"""Visualiser l'erreur entre les notes réelles et les scores prédits"""

import seaborn as sns

# Calcul de l'erreur de prédiction
reviews_data['prediction_error'] = reviews_data['rating'] - reviews_data['predicted_score']

# Création du plot des erreurs (résidus)
plt.figure(figsize=(8, 6))
sns.histplot(reviews_data['prediction_error'], kde=True, color='green', bins=10)

# Ajouter des titres et des labels
plt.title('Distribution des Erreurs de Prédiction', fontsize=16)
plt.xlabel('Erreur (Note Réelle - Score Prédit)', fontsize=12)
plt.ylabel('Fréquence', fontsize=12)

# Affichage du plot
plt.show()

"""Visualisation de la distribution des erreurs de prédiction par catégorie de note réelle"""

# Création du box plot des erreurs par note réelle
plt.figure(figsize=(8, 6))
sns.boxplot(x='rating', y='prediction_error', data=reviews_data, color='lightblue')

# Ajouter des titres et des labels
plt.title('Erreurs de Prédiction par Note Réelle', fontsize=16)
plt.xlabel('Note Réelle', fontsize=12)
plt.ylabel('Erreur (Note Réelle - Score Prédit)', fontsize=12)

# Affichage du plot
plt.show()